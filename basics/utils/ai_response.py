from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage
from decouple import config
import jinja2
import pdfkit
import os


def ai_generated_report(
    user_message: str
) -> str:
    """
    Processes a user's message and retrieves a response from Mixtral.

    This function takes a user's message as input, sends it to the Mixtral system, and then returns the system's response. The function is designed to interact with the Mixtral service, which processes natural language input and generates corresponding responses. The communication with Mixtral and the nature of the response are dependent on Mixtral's capabilities and configuration.

    Parameters:
    user_message (str): A string containing the user's message or query to be sent to Mixtral.

    Returns:
    str: The response generated by Mixtral based on the user's message.

    Note:
    - The function assumes that Mixtral is set up and accessible.
    - The response's accuracy and relevance depend on Mixtral's AI capabilities and the clarity of the user's message.
    - Network or service-related issues may affect the function's ability to communicate with Mixtral.
    """

    api_key = config("MISTRAL_API_KEY")
    model = "open-mixtral-8x7b"

    client = MistralClient(api_key=api_key)

    bot_system_prompt = """You are an intelligent AI assistant. Your works is to help write reports on\
        a variety of topics. Write a detailed report as per user question.
        
        Your responses should be formatted in HTML document following the rules as listed below:
        
        - <h2> tags For headers and <h4> tags for sub-headers
        - <ul> tags for unordered listings and <ol> for ordered listings
        - <b> tag for bolding of text
        - Follow all the other HTML syntax
    """

    # Text QA Prompt
    messages = [
        ChatMessage(
            role="system",
            content=(
                f"{bot_system_prompt} \n\n"
                "The following is a list of conversations between you and the human user\n"
            ),
        ),
        ChatMessage(
            role="user",
            content=(
                "I am a human, answer my question in a very detailed, helpful and friendly manner.\n"
                "---------------------\n"
                f"My question: {user_message}\n"
                "---------------------\n"
                "Answer: "
            ),
        ),
    ]

    try:
        # No streaming, you can use async here
        chat_response = client.chat(
            model=model,
            messages=messages,
        )

        ai_generated_content = chat_response.choices[0].message.content
    except Exception as e:
        # TODO: Improve error handling here
        print(e)
        return ""

    return ai_generated_content
